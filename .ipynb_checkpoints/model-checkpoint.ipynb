{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Data is from here https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus/data, download ner_dataset.csv from the ZIP archive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sentence = 'While speaking on Channels Television on Thursday April 5 2018 Adesina said the fund is not just to intensify the military fight against Boko Haram but to fight other forms of insecurity in the country'\n",
    "\n",
    "validation_tags = ['O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-TIM', 'I-TIM', 'I-TIM', 'I-TIM', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
    "                  'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we parse the file to load sentences and tags into different lists. Also, we only want sentences not more than 35 words long (same length as our validation sentence above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tags = []\n",
    "max_length = len(validation_tags)\n",
    "\n",
    "with open('data/ner_dataset.csv', 'rb') as csvfile:\n",
    "    ner_data = csv.reader(csvfile, delimiter=',')\n",
    "    sentence = []\n",
    "    tag = []\n",
    "    for row in ner_data:\n",
    "        \n",
    "        sentence.append(row[1])\n",
    "        tag.append(row[3].upper())\n",
    "        \n",
    "        if row[1] == '.':\n",
    "            if len(sentence) <= max_length:\n",
    "                sentences.append(sentence)\n",
    "                tags.append(tag)\n",
    "            sentence = []\n",
    "            tag = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is sample entries of `sentences` and `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Word', 'Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.'], ['Families', 'of', 'soldiers', 'killed', 'in', 'the', 'conflict', 'joined', 'the', 'protesters', 'who', 'carried', 'banners', 'with', 'such', 'slogans', 'as', '\"', 'Bush', 'Number', 'One', 'Terrorist', '\"', 'and', '\"', 'Stop', 'the', 'Bombings', '.']]\n",
      "\n",
      "[['TAG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GEO', 'O', 'O', 'O', 'O', 'O', 'B-GEO', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print sentences[:2]\n",
    "print\n",
    "print tags[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create a vocabulary from our sentences i.e a set of unique words. We'll do same for the tags too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = list(set(t for tagset in tags for t in tagset))\n",
    "vocabulary = list(set(word for sentence in sentences for word in sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-ART', 'I-EVE', 'B-EVE', 'B-GPE', 'B-TIM', 'I-TIM', 'B-ORG', 'B-ART', 'O', 'B-GEO', 'I-GPE', 'TAG', 'I-GEO', 'B-PER', 'I-PER', 'I-ORG', 'I-NAT', 'B-NAT']\n"
     ]
    }
   ],
   "source": [
    "print unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heavily-fortified', 'mid-week', '1,800', 'Pronk', 'woods', 'Safarova', 'Nampo', 'hanging', 'trawling', 'five-nation']\n",
      "Number of words in vocabulary 33105\n"
     ]
    }
   ],
   "source": [
    "print vocabulary[:10]\n",
    "print 'Number of words in vocabulary', len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = sentences[:int(.7 * len(sentences))]\n",
    "train_tags = tags[:int(.7 * len(tags))]\n",
    "\n",
    "test_sentences = sentences[int(.7 * len(tags) + 1):]\n",
    "test_tags = tags[int(.7 * len(tags) + 1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31732, 13599, 45332)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(test_sentences), len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "Simple LSTM network with a softmax at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "target_size = len(unique_tags)\n",
    "display_size = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_features = len(vocabulary)\n",
    "sequence_length = 10\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder('float', [None, max_length, n_features], name='X')\n",
    "Y = tf.placeholder('float', [None, max_length, target_size], name='Y')\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, target_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([target_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(n_hidden, state_is_tuple=True)\n",
    "\n",
    "val, state = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(last, weights['out']) + biases['out'])\n",
    "prediction = tf.reshape(prediction, [batch_size, -1, target_size])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(prediction), [1, 2]))\n",
    "\n",
    "# prediction = tf.matmul(last, weights['out']) + biases['out']\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=Y))\n",
    "\n",
    "minimize = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "mistakes = tf.not_equal(tf.argmax(Y, 1), tf.argmax(prediction, 1))\n",
    "error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 469\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_batches = int(len(train_sentences)) / batch_size\n",
    "epoch = 1\n",
    "print 'Number of batches:', num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15024"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run graph using one-hot encoding of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 0 87.09978\n",
      "Loss at batch 50 9.664997\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epoch):        \n",
    "        \n",
    "        for j in range(num_batches):\n",
    "            ptr = 0\n",
    "            batch_X = []\n",
    "            batch_Y = []\n",
    "            for _ in range(batch_size):\n",
    "                x, y = (train_sentences[ptr: ptr + 1], \n",
    "                        train_tags[ptr: ptr + 1])            \n",
    "\n",
    "                x_one_hot = []\n",
    "\n",
    "                for s in x[0]:\n",
    "                    x_one_hot.append(np.eye(len(vocabulary))[vocabulary.index(s)])\n",
    "                    \n",
    "                for remainder in range(max_length - len(x_one_hot)):\n",
    "                    x_one_hot.append([0]*len(vocabulary))\n",
    "                    \n",
    "                batch_X.append(x_one_hot)              \n",
    "\n",
    "                y_one_hot = []\n",
    "\n",
    "                for t in y[0]:\n",
    "                    y_one_hot.append(np.eye(target_size)[unique_tags.index(t)])\n",
    "                    \n",
    "                for remainder in range(max_length - len(y_one_hot)):\n",
    "                    y_one_hot.append(np.eye(target_size)[unique_tags.index('O')])\n",
    "                    \n",
    "                batch_Y.append(y_one_hot)\n",
    "\n",
    "                ptr += 1\n",
    "            \n",
    "            _, entropy, preds = sess.run([minimize, cross_entropy, prediction],{X: np.array(batch_X).reshape(batch_size, max_length, len(vocabulary)), Y: np.array(batch_Y).reshape(batch_size, max_length, target_size)})\n",
    "            \n",
    "            if j % display_size == 0:\n",
    "                print 'Loss at batch {0}'.format(j), entropy\n",
    "            \n",
    "#             valid_x_one_hot = []\n",
    "            \n",
    "#             for v in validation_sentence.split(' ')[:10]:\n",
    "#                 try:\n",
    "#                     valid_x_one_hot.append(np.eye(len(vocabulary))[vocabulary.index(v)])\n",
    "#                 except:\n",
    "#                     valid_x_one_hot.append([0]*len(vocabulary))\n",
    "\n",
    "#             valid_y_one_hot = []\n",
    "\n",
    "#             for vt in validation_tags[:10]:\n",
    "#                 valid_y_one_hot.append(np.eye(target_size)[unique_tags.index(vt)])\n",
    "            \n",
    "#             preds = sess.run([prediction],{X: np.array(valid_x_one_hot).reshape(1, 10, len(vocabulary)), Y: np.array(valid_y_one_hot).reshape(1, 10, 18)})\n",
    "#             print preds\n",
    "\n",
    "        print \"Epoch \",str(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "We'll use Google's word2vec which you can grab from here https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit.\n",
    "To load the word embeddings, we'll neeed another tool, `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word vectors like so. This operations takes a good while on my laptop; core i5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('/Users/h/Projects/Machine-Learning/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is what `boy` is represented according to the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.35351562e-01,  1.65039062e-01,  9.32617188e-02, -1.28906250e-01,\n",
       "        1.59912109e-02,  3.61328125e-02, -1.16699219e-01, -7.32421875e-02,\n",
       "        1.38671875e-01,  1.15356445e-02,  1.87500000e-01, -2.91015625e-01,\n",
       "        1.70898438e-02, -1.84570312e-01, -2.87109375e-01,  2.54821777e-03,\n",
       "       -2.19726562e-01,  1.77734375e-01, -1.20605469e-01,  5.39550781e-02,\n",
       "        3.78417969e-02,  2.49023438e-01,  1.76757812e-01,  2.69775391e-02,\n",
       "        1.21093750e-01, -3.51562500e-01, -5.83496094e-02,  1.22070312e-01,\n",
       "        5.97656250e-01, -1.60156250e-01,  1.08398438e-01, -2.40478516e-02,\n",
       "       -1.16699219e-01,  3.58886719e-02, -2.37304688e-01,  1.15234375e-01,\n",
       "        5.27343750e-01, -2.18750000e-01, -4.54101562e-02,  3.30078125e-01,\n",
       "        3.75976562e-02, -5.51757812e-02,  3.26171875e-01,  6.74438477e-03,\n",
       "        3.71093750e-01,  3.68652344e-02,  6.68945312e-02,  5.17578125e-02,\n",
       "       -4.76074219e-02, -7.91015625e-02,  4.46777344e-02,  1.67968750e-01,\n",
       "        5.51757812e-02, -2.91015625e-01,  2.59765625e-01, -1.00097656e-01,\n",
       "       -1.09863281e-01, -9.15527344e-03,  2.63671875e-02, -3.44238281e-02,\n",
       "        9.37500000e-02,  3.53515625e-01,  8.39843750e-02, -7.75146484e-03,\n",
       "        8.64257812e-02, -5.24902344e-02, -5.59082031e-02, -8.59375000e-02,\n",
       "        5.37109375e-02, -1.47094727e-02,  3.63769531e-02,  4.68750000e-02,\n",
       "       -3.39843750e-01,  1.28906250e-01, -1.22558594e-01,  4.57031250e-01,\n",
       "        1.27929688e-01, -2.89062500e-01,  1.56250000e-01,  3.73535156e-02,\n",
       "        2.75390625e-01, -1.28784180e-02, -1.50390625e-01, -1.64062500e-01,\n",
       "       -3.39843750e-01,  8.00781250e-02, -9.21630859e-03,  2.78320312e-02,\n",
       "        9.32617188e-02,  2.25830078e-02, -1.62353516e-02, -8.25195312e-02,\n",
       "       -1.90429688e-02, -3.49121094e-02,  9.42382812e-02,  3.66210938e-02,\n",
       "        6.39648438e-02,  2.00195312e-01, -4.05273438e-02, -1.08886719e-01,\n",
       "       -3.93676758e-03, -2.55859375e-01,  6.78710938e-02, -1.89453125e-01,\n",
       "        1.72851562e-01, -1.73828125e-01,  2.07031250e-01, -1.59179688e-01,\n",
       "        2.85339355e-03, -1.80664062e-01, -6.93359375e-02,  2.05078125e-01,\n",
       "        5.93261719e-02, -2.17773438e-01, -1.36718750e-01, -4.91333008e-03,\n",
       "       -1.38671875e-01, -7.47070312e-02, -3.54003906e-02,  1.13769531e-01,\n",
       "        3.07617188e-02, -1.05957031e-01, -3.30078125e-01, -2.72216797e-02,\n",
       "       -1.94091797e-02,  9.52148438e-02,  8.69140625e-02, -2.16064453e-02,\n",
       "       -6.98242188e-02, -1.73828125e-01, -1.60156250e-01, -2.44140625e-01,\n",
       "        9.82666016e-03,  2.24609375e-02, -2.13867188e-01,  1.91406250e-01,\n",
       "        2.01171875e-01,  2.72216797e-02,  2.81982422e-02,  2.42187500e-01,\n",
       "        3.55468750e-01, -5.32226562e-02,  1.78710938e-01,  6.78710938e-02,\n",
       "       -6.73828125e-02,  3.49609375e-01, -1.92382812e-01, -1.00097656e-02,\n",
       "       -2.05078125e-01, -1.59179688e-01,  3.76953125e-01, -2.15820312e-01,\n",
       "       -2.36328125e-01,  6.49414062e-02, -1.39770508e-02,  4.22363281e-02,\n",
       "        2.51464844e-02, -1.00585938e-01,  1.37695312e-01, -2.43164062e-01,\n",
       "        1.20605469e-01,  2.03857422e-02,  3.12500000e-01,  1.09863281e-01,\n",
       "       -1.04980469e-01, -9.13085938e-02,  2.21679688e-01, -1.04003906e-01,\n",
       "        1.25976562e-01,  5.10253906e-02,  6.39648438e-02, -1.15722656e-01,\n",
       "       -3.19824219e-02, -8.34960938e-02, -1.97265625e-01, -2.33154297e-02,\n",
       "        1.94335938e-01,  2.24609375e-01, -2.30468750e-01,  4.17480469e-02,\n",
       "        6.49414062e-02, -1.70898438e-01,  7.86132812e-02, -3.58886719e-02,\n",
       "       -1.66015625e-01,  2.25585938e-01,  1.23535156e-01,  1.08398438e-01,\n",
       "        1.15722656e-01,  7.37304688e-02, -1.56250000e-02, -5.85937500e-02,\n",
       "       -8.93554688e-02,  1.30859375e-01,  1.90429688e-01, -3.58886719e-02,\n",
       "       -1.36718750e-02, -1.88476562e-01, -1.48437500e-01, -2.51953125e-01,\n",
       "       -1.22558594e-01, -2.75390625e-01, -1.54296875e-01, -2.83203125e-01,\n",
       "        1.10839844e-01, -2.46093750e-01,  1.89453125e-01, -2.50244141e-02,\n",
       "        8.59375000e-02, -1.17675781e-01, -2.46582031e-02, -1.32812500e-01,\n",
       "        1.00097656e-01, -2.45117188e-01, -2.02148438e-01, -7.56835938e-02,\n",
       "        6.03027344e-02,  1.72851562e-01, -6.59179688e-02,  6.78710938e-02,\n",
       "        6.98242188e-02, -4.10156250e-02,  2.14843750e-01,  7.17773438e-02,\n",
       "       -4.57763672e-03, -4.04357910e-04,  8.59375000e-02, -2.55859375e-01,\n",
       "       -4.32128906e-02, -1.31835938e-01,  2.05078125e-02, -2.46093750e-01,\n",
       "       -1.28906250e-01,  1.23535156e-01, -1.48437500e-01,  5.15136719e-02,\n",
       "       -1.55273438e-01, -1.70898438e-01,  1.92382812e-01,  2.16796875e-01,\n",
       "        5.81054688e-02, -1.28906250e-01, -1.43554688e-01, -7.78198242e-03,\n",
       "       -1.15234375e-01,  4.08203125e-01, -3.37890625e-01,  8.64257812e-02,\n",
       "        2.08007812e-01,  2.35595703e-02,  1.36718750e-01, -4.71191406e-02,\n",
       "        9.91210938e-02,  1.18164062e-01,  1.19140625e-01,  1.24511719e-01,\n",
       "        4.66308594e-02,  5.41992188e-02, -2.11914062e-01, -8.20312500e-02,\n",
       "       -5.17578125e-02,  2.03857422e-02, -1.59179688e-01, -1.76757812e-01,\n",
       "        8.54492188e-02,  1.38671875e-01, -1.01562500e-01,  2.61230469e-02,\n",
       "       -1.88476562e-01, -1.57470703e-02,  1.21093750e-01, -9.66796875e-02,\n",
       "        2.13623047e-02, -6.68945312e-02, -2.69775391e-02,  3.51562500e-02,\n",
       "        1.68945312e-01,  1.55639648e-02, -1.25976562e-01, -1.44531250e-01,\n",
       "        1.78710938e-01, -7.42187500e-02,  2.72216797e-02,  4.98046875e-01,\n",
       "       -6.03027344e-02, -1.35742188e-01, -1.62109375e-01,  9.57031250e-02,\n",
       "       -1.84326172e-02,  3.90625000e-01,  1.90429688e-02, -1.03149414e-02,\n",
       "       -1.15234375e-01, -2.91015625e-01, -5.95703125e-02, -5.37109375e-02,\n",
       "       -7.42187500e-02, -2.65625000e-01, -1.03027344e-01,  1.35742188e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.word_vec('boy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run graph with words represented as word2vec\n",
    "Same as architecture as pervious except `n_features` is now the dimension of the vector returned by word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "target_size = len(unique_tags)\n",
    "display_size = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_features = 300 # dimension of the vector return by word2vec\n",
    "sequence_length = max_length\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder('float', [None, max_length, n_features], name='X')\n",
    "Y = tf.placeholder('float', [None, max_length, target_size], name='Y')\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, target_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([target_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(n_hidden, state_is_tuple=True)\n",
    "\n",
    "val, state = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(last, weights['out']) + biases['out'])\n",
    "prediction = tf.reshape(prediction, [batch_size, -1, target_size])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(prediction), [1, 2]))\n",
    "\n",
    "# prediction = tf.matmul(last, weights['out']) + biases['out']\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=Y))\n",
    "\n",
    "minimize = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "mistakes = tf.not_equal(tf.argmax(Y, 1), tf.argmax(prediction, 1))\n",
    "error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 991\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "num_batches = int(len(train_sentences)) / batch_size\n",
    "epoch = 1\n",
    "print 'Number of batches:', num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 0 101.00055\n",
      "Loss at batch 50 15.769457\n",
      "Loss at batch 100 13.819975\n",
      "Loss at batch 150 13.336898\n",
      "Loss at batch 200 12.802603\n",
      "Loss at batch 250 12.793713\n",
      "Loss at batch 300 12.767519\n",
      "Loss at batch 350 12.957819\n",
      "Loss at batch 400 12.76519\n",
      "Loss at batch 450 12.762266\n",
      "Loss at batch 500 12.815669\n",
      "Loss at batch 550 12.763356\n",
      "Loss at batch 600 12.761274\n",
      "Loss at batch 650 12.760481\n",
      "Loss at batch 700 12.777012\n",
      "Loss at batch 750 12.76202\n",
      "Loss at batch 800 12.76059\n",
      "Loss at batch 850 12.760016\n",
      "Loss at batch 900 12.79056\n",
      "Loss at batch 950 12.762553\n",
      "Epoch  0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epoch):        \n",
    "        \n",
    "        for j in range(num_batches):\n",
    "            ptr = 0\n",
    "            batch_X = []\n",
    "            batch_Y = []\n",
    "            for _ in range(batch_size):\n",
    "                x, y = (train_sentences[ptr: ptr + 1], \n",
    "                        train_tags[ptr: ptr + 1])            \n",
    "\n",
    "                x_one_hot = []\n",
    "\n",
    "                for s in x[0]:\n",
    "                    try:\n",
    "                        x_one_hot.append(w2v.word_vec(s))\n",
    "                    except:\n",
    "                        #if word isn't in the word2vec, use zeroes\n",
    "                        x_one_hot.append([0]*n_features)\n",
    "                    \n",
    "                for remainder in range(max_length - len(x_one_hot)):\n",
    "                    #pad sentence remainder with zeroes\n",
    "                    x_one_hot.append([0]*n_features)\n",
    "                    \n",
    "                batch_X.append(x_one_hot)              \n",
    "\n",
    "                y_one_hot = []\n",
    "\n",
    "                for t in y[0]:\n",
    "                    y_one_hot.append(np.eye(target_size)[unique_tags.index(t)])\n",
    "                    \n",
    "                for remainder in range(max_length - len(y_one_hot)):\n",
    "                    y_one_hot.append(np.eye(target_size)[unique_tags.index('O')])\n",
    "                    \n",
    "                batch_Y.append(y_one_hot)\n",
    "\n",
    "                ptr += 1\n",
    "            \n",
    "            _, entropy, preds = sess.run([minimize, cross_entropy, prediction],{X: np.array(batch_X).reshape(batch_size, max_length, n_features), Y: np.array(batch_Y).reshape(batch_size, max_length, target_size)})\n",
    "            \n",
    "            if j % display_size == 0:\n",
    "                print 'Loss at batch {0}'.format(j), entropy\n",
    "\n",
    "        print \"Epoch \",str(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvious benefit of using word2vec is that the network runs faster, converges quicker too. Runs faster because we've reduced the feature representation from an outrageous dimension in the length of the vocabulary (thousands) to only 300, the dimension of the array returned by word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 18 values, but the requested shape requires a multiple of 576\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Softmax, Reshape/shape)]]\n\nCaused by op u'Reshape', defined at:\n  File \"/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-6f58bb698d38>\", line 11, in <module>\n    prediction = tf.reshape(prediction, [batch_size, -1, target_size])\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2510, in reshape\n    name=name)\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 18 values, but the requested shape requires a multiple of 576\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Softmax, Reshape/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d1ded585f4cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mvalid_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 18 values, but the requested shape requires a multiple of 576\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Softmax, Reshape/shape)]]\n\nCaused by op u'Reshape', defined at:\n  File \"/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-6f58bb698d38>\", line 11, in <module>\n    prediction = tf.reshape(prediction, [batch_size, -1, target_size])\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2510, in reshape\n    name=name)\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 18 values, but the requested shape requires a multiple of 576\n\t [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Softmax, Reshape/shape)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    valid_X = []\n",
    "    \n",
    "    for word in validation_sentence.split(' '):\n",
    "        try:\n",
    "            valid_X.append(w2v.word_vec(word))\n",
    "        except:\n",
    "            #if word isn't in the word2vec, use zeroes\n",
    "            valid_X.append([0]*n_features)\n",
    "\n",
    "    for remainder in range(max_length - len(valid_X)):\n",
    "        #pad sentence remainder with zeroes\n",
    "        valid_X.append([0]*n_features)           \n",
    "\n",
    "    valid_Y = []\n",
    "\n",
    "    for t in validation_tags:\n",
    "        valid_Y.append(np.eye(target_size)[unique_tags.index(t)])\n",
    "\n",
    "    for remainder in range(max_length - len(valid_Y)):\n",
    "        valid_Y.append(np.eye(target_size)[unique_tags.index('O')])\n",
    "            \n",
    "    preds = sess.run([prediction],{X: np.array(valid_X).reshape(1, max_length, n_features), Y: np.array(valid_Y).reshape(1, max_length, target_size)})\n",
    "    print preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to try\n",
    "- Use a bidirectional LSTM\n",
    "- Add dropout\n",
    "- Replace softmax with Linear-Chain CRF\n",
    "- Try other word representations; Glove?\n",
    "- Tune batch size, learning rate\n",
    "- Add MOAR layers!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
